你有什么优势，说一下吧

## **自我介绍**

- 姓名：邵桐杰-2023年6月份毕业，在读广东白云学院-本科-软件工程专业。两份实习，第一家公司是新原子科技有限公司，它是一家金融行业实习，第二家公司叫智腾达，它是一家物流系统ERP实习。奖项，拿过国家励志奖学金，一等奖奖学金
- 目前我的技术栈是Java语言用的比较多，从Java基础-Javaweb-ssm-springboot-springcloud这一条路线有系统学过
- linux方面：我有自己的腾讯云和阿里云服务器，使用过从原生的Java项目部署，打包成一个war包，到springboot的jar包,到编写dockerfile部署，到docker+Jenkins实现devops的cicd流程，到k8s集群部署等有写过一遍demo。
- 数据库有使用MySQL，redis
- ⼯作之余，我喜欢总结与分享，我有自己的博客的公众号，公众号有700粉丝，平时也有参与开源项目的pr。⽣活中我是⼀个比较积极乐观的⼈，⼀般会通过运动打球的⽅式来放松。我⼀直都⾮常想加⼊贵公司，我觉得贵公司的⽂化 和技术氛围我都⾮常喜欢，期待能与你共事！

# **深圳智腾达**

**主要负责后端接口开发+测试+运维部署工作**

1. ## 痛点一：改造系统架构

- **痛点原因**：
  - 停服务更新，影响用户体验。
  - 单体架构，高峰期对服务器带来压力。
- 分析痛点
  - 方案一：搭建docker+Jenkins实现cicd自动化部署。这个方案成本很高(领导不采纳)，项目不是很大，所以采用配置脚本+项目方式。
  - 方案二：搭配nginx+脚本去部署。
- 解决痛点：
  - 用nginx作为负载均衡是为了解决两个问题，
    1. 第一：是减轻服务器的压力
    2. 第二：解决在不停服务的条件下进行更新和维护。

1. 当时的背景是这样，项目是单体项目。在中午高峰期的时候，用户的访问量也比较大嘛，而这个时候客户要进行后端项目更新与维护，所以只能和用户说一下暂停使用，所以这个对用户体验是非常不友好的。
2. 所以我当时去调研一下，如何才能不停服务的去更新部署项目。所以采用了nginx作为负载均衡，启动4台服务器，分别设置4个不同端口17771，17772，17773，17774。不过是部署在同一台服务器上的，算是一个伪的分布式。所以这样一来，我每一次写完代码之后，自己测试，然后自己部署，先暂停一个端口17771嘛，再把项目丢到服务器上，改一下项目名称17771，启动17771脚本即可，一定要等到17771项目启动完成之后，就逐个替代其他项目。所以这样以来，用户无需等待使用，可以每时每刻进行使用。如果有问题的话就回滚版本。当然，项目到了后面就不选择在高峰期更新。

![img](https://bytedancecampus1.feishu.cn/space/api/box/stream/download/asynccode/?code=NDBhM2RkMWNhZWZlNWU1ZGY2ZGQ4YjMwY2YzMjY1NWVfOWp1enNpbUVOQk1sbzNwZ1JhNHlCRFB2OWtCd3pvWklfVG9rZW46Ym94Y25JUTdQeUNPUk11SEZzcU5sa1hoTzljXzE2ODgzNTg4NjM6MTY4ODM2MjQ2M19WNA)

1. 后面我还去调研了解到，还可以通过docker+Jenkins实现cicd自动化部署。这个主要核心在于Jenkins。这个是怎么实现的呢，我说一个大体思路：首先这个流程是这样子的，我们开发人员将代码提交到Gitlab,然后通过人工/定时触发（开启轮询SCM）项目构建。接着Jenkins去拉取代码、代码编译、打包镜像、推送到harbor镜像仓库，最后docker主机创建容器并发布。同时Jenkins集成钉钉或者是企业微信飞书等，成功了之后就推送到钉钉上。等这一步完成之后，再引入代码分析平台SonarQube，可以检测出代码漏洞、代码规范和安全性漏洞的问题。后期引入k8s进行容器编排处理。

1. ## **处理线上问题。**

**线上问题出现了，如何解决？**

1. 复现问题。先看一下用户截图反馈的是什么错误，先对问题进行复现。是前端还是后端的问题。
2. 如果是后端的问题。好，分析线上日志。通过**用户截图的时间点**，定位到线上日志。看看是具体什么问题？
   1. 是接口逻辑吗？那就通过本地debug一遍，
   2. null指针，没有赋值；
   3. 字段没显示，mybatis映射问题。
3. 为了保证减少线上出现问题，对接口维护进行一些处理。
   1. 写单测。即使下一次业务有更改，可以及时定位到原来的业务代码段，进行修复。
   2. postman进行接口可行性测试。
   3. 前后端进行本地联调。如果是上线的条件下。为了避免出错，前端比较忙，拿前端的项目和后端一起本地跑起来。进行联调。

1. ## 缓存不一致问题

redis缓存不一致出现原因

当时怎么处理

后期怎么避免缓存不一致问题

##### 延时双删

最简单的解决办法延时双删

```Java
public void write(String key,Object data) {
    Redis.delKey(key);
    db.updateData(data) ;
    Thread.sleep(1000) ;
    Redis.delkey(key);
}
```

转化为中文描述就是

1. 先删缓存
2. 再写数据库(这两步和原来一样)
3. 休眠1秒，再删缓存，这么做，可以将1秒内所造成的缓存脏数据，再次删除。确保读请求结束，写请求可以删除读请求造成的缓存脏数据。自行评估自己的项目的读数据业务逻辑的耗时，写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可。

1. ## 第三方接口开发

痛点分析

1. 原来快递员是在公众号下单，后面迁移到了小程序，前端原来是安卓写的，后面安卓的难招，迁移到了uniapp开发。
2. 第三方绑定快递员。

# **新原子科技：**

**主要负责订单模块的接口维护+单元测试**

## 痛点：解决订单模块的业务逻辑的准确性

1. ## mock框架调研 JUnit、JMockit、Mockito、PowerMockito区别

**痛点**：我们要验证交易结算后端接口测试的业务逻辑的准确性这个。如果是涉及到简单的mapper层接口可以解决，但是涉及到复杂的业务接口就需要用到mock框架去做，所以我们当时去市面上对一些mock框架做了一些调研。比如**JUnit、JMockit、Mockito、PowerMock。**

调研之后发现，

1. **JUnit是一个开源的java单元测试框架。**涉及到比较复杂的类或对运行环境有要求的类的单元测试，模拟环境或者配置环境会非常耗时，实施单元测试比较困难。
2. **jmock**通过mock对象来模拟一个对象的行为，我们不关心的其他对象，使得测试变得简单。
   1. 缺点：Jmock需要在**执行前记录期望行为**（expectations），显得很繁琐，
3. **Mockito**通过在执行后校验哪些函数已经被调用，解决了对期望行为（expectations）的需要，API非常简洁。

```Java
Mockito用法  
when(mock.someMethod()).thenReturn(value):设定mock对象某个方法调用时的返回值。
可以连续设定返回值，即when(mock.someMethod()).thenReturn(value1).then Return(value2),
第一次调用时返回value1,第二次返回value2。也可以表示为如下： 
when(mock.someMethod()).thenReturn(value1，value2)。
```

1. Mockito**只能对public的方法进行Mock**，而PowerMock可以实现完成对**private/static/final**方法的Mock（模拟）。相比于 mockito，PowerMock 在兼容 mockito 的基础上，增加了许多特殊的 mock 操作。Mockito解决不了一些mock对象是方法内新生成的，从方法外部将mock的对象传递到方法内时。而**PowerMock却可以解决上述的各种问题**。**Mockito模拟不了的方法，可以通过PowerMock去模拟**

PowerMock各方法的语法(**一般方法与Mockito用法一致**)： 

1. void静态：
   1. PowerMockito.mockStatic(xxx.Class); 
   2. PowerMockito.doNothing().when(mock,”methodName”,arg1,arg2); 
2. 有返回值静态：
   1. PowerMockito.mockStatic(xxx.Class);              
   2. PowerMockito.when(mock.method()).thenReturn(value); 
3. 私有：
   1. PowerMockito.mock(xxx.Class);       
   2. PowerMockito.when(mock,”methodName”,arg1,arg2).thenReturn(value);

最后，调研的结果就是通过**Junit4+PowerMock结合一起使用，解决了绝大部分的接口业务逻辑测试的业务场景。打桩-验证。**

1. powermock框架说一下。PowerMock是一个Java模拟框架，用于解决测试问题。里面有@Spy和 @SpyBean的区别， @Mock 和 @MockBean的区别
   1. spy和mock生成的对象不受spring管理
   2. SpyBean和MockBean生成的对象受spring管理，相当于自动替换对应类型bean的注入，比如 @Autowired等注入
   3. spy调用真实方法时，其它bean是无法注入的，要使用注入，要使用SpyBean

1. 你们写了这么多测试用例，万一业务有变更，那你的单测也要变更，那怎么办呢。如果在高并发的场景下，怎么保证你的单测呢？

- 第一种：对简单业务的接口进行单测（加事务@Transactional）；第二种，模拟用户发起请求，查看断言数据。
  - 1，准备假数据，通过业务（mapper层）写好的接口进行插入，然后处理逻辑，模拟发起mock查询请求。获取响应结果。断言各个字段是否和查询的数据是否一致。（插入的数据和你查询到的数据是否为同一个）
- 查询：query
  - 准备假数据，进行插入，调用查询接口，看看是否插入成功。
  - 测查询所有findAll，准备两条假数据进行插入，调用查询接口，判断数据是否等于2。
- 更新：update
  - 准备假数据进行插入，修改实体的字段，调用更新接口进行更新。断言更新后的状态。
  - 更新几条数据updateList：准备几条数据进行插入。修改这几个实体字段。调用更新接口updateList进行更新，对每条数据更新的是状态进行断言验证。
- 准备数据 --> 参数校验 --> 断言

1. ## 压力测试的原因是什么，为什么要压力测试

保障整个系统的稳定性

压力测试其实有两个目的，

1. 一是检查高并发下代码是否会报错。 是否存在内存泄漏。 
2. 二是测**应用的抗压能力**，预估应用的承载能力，为后面的运维提供扩容的依据。

按照**100、200、300、500进程数进行****压力测试**，压到500如果没有报错就可以进行**疲劳测试**，观察内存占用。

https://blog.csdn.net/weixin_39362573/article/details/127163169

1. ## 火焰图是什么

    async-profiler火焰图是Java 性能分析工具

   1. 当时我们的工作是利用火焰图分析CPU耗时，存在的性能瓶颈，进行io层序列化优化。
   2. 具体步骤是取30s内生成svg文件，有x轴和y轴，x轴为调用顺序，y轴为栈深。一般来说需要关注的就是栈顶，且宽度比较大的那个。而且处于栈顶的，说明其存在性能问题

1. ## **序列化和反序列化是什么。如何进行优化**

1. Java 序列化是指把 Java 对象转换为字节序列的过程；Java 反序列化是指把字节序列恢复为Java 对象的过程；
2. 序列化目的有两个：对象持久化、网络传输
   1. 对象持久化。**实现了数据的持久化，通过序列化可以把数据永久的保存在硬盘上；**
   2. 网络传输。**利用序列化实现远程通信，即在网络上传递对象的字节序列。**（序列化与反序列化则实现了 进程通信间的对象传送，发送方需要把这个Java对象转换为字节序列，才能在网络上传送；接收方则需要把字节序列再恢复为Java对象）

**io****层序列化优化**

- 自定义实现序列化组件`MessagePack`的`write`和`read`方法实现io层序列化与反序列化。

为什么采用字节序列，有什么依据？

采用了**MessagePack**怎么自定义实现呢？

### 序列化与反序列化框架优劣时，考虑几个因素:

1. 开发工作量和难度。**Protobuf就需要学习成本**
2. 编码后的码流大小，性能怎么样。jdk和json就不适合
3. 是否支持跨语言，支持的语言种类是否丰富。

**编码后的字节数组越大，存储占空间，存储硬件成本高，网络传输时也占带宽，导致系统的吞吐量降低。**Java序列化后的码流偏大也一直被业界所诟病，导致它的应用范围受到了很大的限制。

## **四种序列化方式：JDK默认序列化，****JSON****，Protobuf，MessagePack**

**JDK默认的序列化机制**。实现Serializable。

1. **序列化后的码流太大：jdk序列化机制编码后的二进制数组大小竟然是二进制编码的5.29倍。**
2. **序列化性能太低**：java序列化的性能只有二进制编码的6.17%左右，Java原生序列化的性能实在太差。

JDK默认的序列化机制很差。所以我们通常不会选择Java默认序列化这种

### **JSON****（****gson****满足不了业务场景）**

- **体积大，影响高并发。性能相对较低，维护成本较高**。无版本检查，自己做兼容

### **Protobuf**

- 对象冗余，字段很多，生成的类较大，占用空间。学习成本较高。

### **MessagePack**

1. 序列化反序列化效率高（比json快一倍），文件体积小，比json小一倍。
2. 兼容json数据格式
3. 跨语言，多语言支持(超多)

![img](https://bytedancecampus1.feishu.cn/space/api/box/stream/download/asynccode/?code=N2QwZmI3ZDgxYjc5MDI4NTM3NzkwNzRiZGQxMTgxZDFfQU95N29OV2dnWkg1OFNMTDhJcFBnY29DOVRESW5aUGJfVG9rZW46Ym94Y25LZGF0TUZyV011TnRXbWpSd3BUNGZkXzE2ODgzNTg4NjM6MTY4ODM2MjQ2M19WNA)

通过配置msgpack的maven依赖

封装MsgPackTemplate抽象类，对原有msgpack进行二次加工，比如int，Short，Byte，BigDecimal进行read处理

然后针对大对象实体类进行读和写的序列化与反序列化操作。

```Java
<dependency>
    <groupId>org.msgpack</groupId>
    <artifactId>msgpack</artifactId>
    <version>0.6.12</version>
</dependency>
package com.atomintl.utils;

import java.io.IOException;
import java.math.BigDecimal;
import java.util.Date;

import org.msgpack.packer.Packer;
import org.msgpack.template.CharacterTemplate;
import org.msgpack.template.Template;
import org.msgpack.unpacker.Unpacker;

public abstract class MsgPackTemplate <T> implements Template<T> {

    public void write(Packer pk, T v) throws IOException {
        write(pk, v, false);
    }

    public T read(Unpacker u, T to) throws IOException {
        return read(u, to, false);
    }

    public BigDecimal readBigDecimal(Unpacker u) throws IOException {
       if (u.trySkipNil()) {
            return null;
        }
       
       String temp = u.readString();
       if (temp != null) {
          return new BigDecimal(temp);
       }
       return null;
    }
    
    public Date readDate(Unpacker u) throws IOException {
       if (u.trySkipNil()) {
            return null;
        }
       
       long temp = u.readLong();
       if (temp > 0) {
          return new Date(temp);
       }
       return null;
    }

    public String readString(Unpacker u) throws IOException {
        if (u.trySkipNil()) {
            return null;
        }
        return u.readString();
    }

    public Integer readInteger(Unpacker u) throws IOException {
        if (u.trySkipNil()) {
            return null;
        }
        return u.readInt();
    }

    public Byte readByte(Unpacker u) throws IOException {
        if (u.trySkipNil()) {
            return null;
        }
        return u.readByte();
    }

    public Short readShort(Unpacker u) throws IOException {
        if (u.trySkipNil()) {
            return null;
        }
        return u.readShort();
    }

    public Float readFloat(Unpacker u) throws IOException {
        if (u.trySkipNil()) {
            return null;
        }
        return u.readFloat();
    }

    public Double readDouble(Unpacker u) throws IOException {
        if (u.trySkipNil()) {
            return null;
        }
        return u.readDouble();
    }

    public Character readCharacter(Unpacker u) throws IOException {
        if (u.trySkipNil()) {
            return null;
        }
        return u.read(CharacterTemplate.getInstance());

    }

    public Long readLong(Unpacker u) throws IOException {
        if (u.trySkipNil()) {
            return null;
        }
        return u.readLong();
    }

    public Boolean readBoolean(Unpacker u) throws IOException {
        if (u.trySkipNil()) {
            return null;
        }
        return u.readBoolean();
    }

    


    public void writeBigDecimal(Packer pk, BigDecimal v) throws IOException {
       if (v == null) {
            pk.writeNil();
        } else {
           pk.write(v.toString());
        }
    }
    
    public void writeDate(Packer pk, Date v) throws IOException {
       if (v == null) {
            pk.writeNil();
        } else {
           pk.write(v.getTime());
        }
    }

    public void writeString(Packer pk, String v) throws IOException {
       if (v == null) {
            pk.writeNil();
        } else {
           pk.write(v);
        }
    }

    public void writeInt(Packer pk, int v) throws IOException {
        pk.write(v);
    }
    
    public void writeInteger(Packer pk, Integer v) throws IOException {
       if (v == null) {
            pk.writeNil();
        } else {
           pk.write(v.intValue());
        }
    }

    public void writeByte(Packer pk, Byte v) throws IOException {
       if (v == null) {
            pk.writeNil();
        } else {
           pk.write(v.byteValue());
        }
    }

    public void writeShort(Packer pk, Short v) throws IOException {
       if (v == null) {
            pk.writeNil();
        } else {
           pk.write(v.shortValue());
        }
    }

    public void writeFloat(Packer pk, Float v) throws IOException {
       if (v == null) {
            pk.writeNil();
        } else {
           pk.write(v.floatValue());
        }
    }

    public void writeDouble(Packer pk, Double v) throws IOException {
       if (v == null) {
            pk.writeNil();
        } else {
           pk.write(v.doubleValue());
        }
    }

    public void writeCharacter(Packer pk, Character v) throws IOException {
       if (v == null) {
            pk.writeNil();
        } else {
           pk.write(v.charValue());
        }

    }

    public void writeLong(Packer pk, Long v) throws IOException {
       if (v == null) {
            pk.writeNil();
        } else {
           pk.write(v.longValue());
        }
    }
    
    public void writeLong(Packer pk, long v) throws IOException {
        pk.write(v);
    }

    public void writeBoolean(Packer pk, Boolean v) throws IOException {
       if (v == null) {
            pk.writeNil();
        } else {
           pk.write(v.booleanValue());
        }
    }
}
package com.atomintl.exchange.master.futures.position.domain;

import com.atomintl.exchange.master.user.domain.UserRate;
import com.atomintl.utils.MsgPackTemplate;
import org.msgpack.packer.Packer;
import org.msgpack.unpacker.Unpacker;
import java.io.IOException;

public class UserRateTemplate extends MsgPackTemplate<UserRate> {
    @Override
    public void write(Packer pk, UserRate v, boolean required) throws IOException {
        if (v == null) {
            pk.writeNil();
            return;
        }
        writeString(pk,v.getId());
        writeString(pk,v.getUserID());
        writeString(pk,v.getType());
        writeBigDecimal(pk,v.getRate());
        writeDate(pk,v.getCreateTime());
        writeDate(pk,v.getLastUpdateTime());
    }

    @Override
    public UserRate read(Unpacker u, UserRate to, boolean required) throws IOException {
        if (!required && u.trySkipNil()) {
            return null;
        }
        if (to == null) {
            to = new UserRate(); //or toBuilder
        }

       to.setId(readString(u));
       to.setUserID(readString(u));
       to.setType(readString(u));
       to.setRate(readBigDecimal(u));
       to.setCreateTime(readDate(u));
       to.setLastUpdateTime(readDate(u));
        return to;
    }
}
```

## 抽象类与接口的区别 为什么用抽象类

1. 抽象类中的成员变量可以被不同的修饰符来修饰，可接口中的成员变量只能是静态常量（static final）
2. 定义成抽象类目的让子类来继承的，因为父类里有很多方法是无法定义具体的实现的，只能定义一个原型，让子类来分别实现！所以要定义成抽象的！

二、抽象类与接口有什么区别？

记住最重要的一点，一个是类，一个是接口（废话），然后开始发散

1. 抽象类里的方法可以有具体实现，而接口不能。（即抽象类中的方法不一定全是抽象方法）
2. 一个类只能继承一个抽象类，却可以实现多个接口。
3. 抽象类可以有静态代码块和静态方法，而接口不能。
4. 抽象类中的成员变量可以被不同的修饰符来修饰，可接口中的成员变量默认的都是静态常量
5. 子类中实现父类中的抽象方法时，可见性可以大于等于父类中的；而接口实现类中的接口 方法的可见性只能与接口中相同（public）

能够实现面向对象设计的一个最核心的原则——开闭原则。那就给予了面试官，你懂一点设计模式的暗示

如果你简单地说：为了重用。减少编码量，降低耦合性。虽然正确，但听起来不是很有感觉。

但如果你能扯到：能够实现面向对象设计的一个最核心的原则——开闭原则。那就给予了面试官，你懂一点设计模式的暗示

如果你直接就跟面试官开始聊，在某个项目中为了降低耦合性，减少编码量，你自己自主重构了代码，使其符合开闭原则。那就是非常具有实践精神，技术思维。（但是要注意这个项目一定不能是你简历上的主力项目，否则面试官会觉得你做过的项目很low）

原文链接：https://blog.csdn.net/with_luck/article/details/119444335

1. 参与性能优化对吧，jvm内存结构说下，gc垃圾回收说一下，jvm调优说一下。

![img](https://bytedancecampus1.feishu.cn/space/api/box/stream/download/asynccode/?code=MWNlNTRmZDg2NjljMjllMDM3NTQ2NmYwNGQ2MWI2YWJfWDduNTNOT1F5WVljdTFyOEVxMEFNTElpTGRFeGtkcnBfVG9rZW46Ym94Y25JWFFiVmh5bUlubEFTbXRJdzdjUVRjXzE2ODgzNTg4NjM6MTY4ODM2MjQ2M19WNA)

# 八股文

## mysql索引，存储引擎，隔离级别，MySQL优化说一下

###  MySQL有哪些存储引擎，`MyISAM`和`InnoDB`的区别有哪些？MySQL常见的存储引擎有：`InnoDB、MyISAM、Memory`。从MySQL5.5开始，InnoDB也是MySQL的默认存储引擎  MyISAM和InnoDB的区别有哪些？InnoDB支持事务，外键，MyISAM不支持事务，外键。InnoDB是聚集索引（数据文件是和索引绑在一起的，必须要有主键，通过主键索引效率很高）；MyISAM是非聚集索引（数据文件是分离的，索引保存的是数据文件的指针，主键索引和辅助索引是独立的）Innodb不支持全文索引，而MyISAM支持全文索引，查询效率上MyISAM要高;InnoDB 不保存表的具体行数，MyISAM 用一个变量保存了整个表的行数。MyISAM采用表级锁(table-level locking)； InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁。MyISAM是适合用于频繁查询的应用，表锁，不会出现死锁，适合小数据，小并发Innodb是适合于插入和更新操作比较多的应用；设计合理的话是行锁(最大区别就在锁的级别上) ；适合大数据，大并发

## 数据库的三大范式有哪些

1. 第一范式：每个列都不可以在拆分。
2. 第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。
3. 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。

##  MySQL事务四大特性ACID

- 原子性(Atomic) 对数据的修改要么全部执行，要么全部不执行。
- 一致性(Consistent) 在事务执行前后，数据状态保持一致性。
- 隔离性(Isolated) 一个事务的处理不能影响另一个事务的处理。
- 持续性(Durable) 事务处理结束，其效果在数据库中持久化。

##  Mysql怎么保证原子性的？

 是利用Innodb的undo log。

 undo log名为回滚日志，是实现原子性的关键，当事务回滚时能够撤销所有已经成功执行的sql语句，他需要记录你要回滚的相应日志信息。

 例如：

 (1)当你delete一条数据的时候，就需要记录这条数据的信息，回滚的时候，insert这条旧数据

 (2)当你update一条数据的时候，就需要记录之前的旧值，回滚的时候，根据旧值执行update操作

 (3)当年insert一条数据的时候，就需要这条记录的主键，回滚的时候，根据主键执行delete操

 undo log记录了这些回滚需要的信息，当事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。

##  Mysql怎么保证持久性的？

 是利用Innodb的redo log。/rIdo/

 正如之前说的，Mysql是先把磁盘上的数据加载到内存中，在内存中对数据进行修改，再刷回磁盘上。如果此时突然宕机，内存中的数据就会丢失。

 怎么解决这个问题？

 简单啊，事务提交前直接把数据写入磁盘就行啊。

 这么做有什么问题？

 只修改一个页面里的一个字节，就要将整个页面刷入磁盘，太浪费资源了。毕竟一个页面16kb大小，你只改其中一点点东西，就要将16kb的内容刷入磁盘，听着也不合理。

 毕竟一个事务里的SQL可能牵涉到多个数据页的修改，而这些数据页可能不是相邻的，也就是属于随机IO。显然操作随机IO，速度会比较慢。

 采用redo log解决上面的问题。当做数据修改的时候，不仅在内存中操作，还会在redo log中记录这次操作。当事务提交的时候，会将redo log日志进行刷盘(redo log一部分在内存中，一部分在磁盘上)。当数据库宕机重启的时候，会将redo log中的内容恢复到数据库中，再根据undo log和binlog内容决定回滚数据还是提交数据。

##  说一下MySQL的四种隔离级别 ：读未提交，读已提交，可重复读，串行化。

 **读取未提交。会导致脏读。**

- 解决办法：1，把隔离级别调整到读已提交；读取时，加共享锁，读取完之后释放事务。2，修改时，加入排他锁，直至事务提交之后才释放。

 **读取已提交。也称不可重复读**：指的是一个事务先后读取同一条记录，但两次读取的数据不同。解决办法：方法1，把事务隔离级别调整到可重复读。方法2：读取数据时加共享锁，写数据时加排他锁，都是事务提交才释放锁。读取时候不允许其他事物修改该数据，不管数据在事务过程中读取多少次，数据都是一致的，避免了不可重复读问题。

 **可重复读，是MySQL的默认事务隔离级别。会出现幻读**：指的是同一个事务中，用同样的的操作读取两次，得到的记录数不相同。解决办法：把事务隔离级别调整到串行化。

 **Serializable (可串行化)**

- 通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。

 MySQL默认采用的可重复读隔离级别；**Oracle默认采用的读已提交**隔离级别事务隔离机制的实现基于锁机制和并发调度。其中并发调度使用的是MVVC (多版本并发控制)，通过保存修改的旧版本信息来支持并发一致性读和回滚等特性。

1. 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是读已提交；但是你要知道的是InnoDB存储引擎默认使用可重读并不会有任何性能损失。
2. InnoDB存储引擎在分布式事务的情况下一般会用到SERIALIZABLE(可串行化)隔离级别。

**MySQL优化**：索引优化、设计优化，硬件优化：MySQL 对硬件优化有三个方面：磁盘（固态硬盘）、网络（设置多个网卡，以提高网络高峰期 MySQL 服务器的运行效率）和内存（加服务器内存）。

1. explain命令来查看语句的执行计划。看看SQL语句的执行的详细情况，是否使用索引。
2. 优化shema、sql语句+索引;
3. 第二加缓存，memcached, redis;
4. 主从复制，读写分离; 
5. 垂直拆分，根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统;
6. 水平切分，针对数据量大的表，这一步最麻烦， 最能考验技术水平，要选择一个合理的sharding key,为了有好的查询效率，表结构也要改动，做一定的冗余， 应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表;

###  如何实现MySQL的读写分离?  简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。

###  MySQL主从复制流程和原理?

 基本原理流程，是3个线程以及之间的关联

1. 第一步：主：binlog线程一一记录下所有改变了数据库数据的语句，放进master上的binlog中; 
2. 第二步：从：io线程一一负责从master上拉取binlog 内容，放进自己的relay log中;
3. 第三步：从：sql执行线程一一执行relay log中的语句;

## **接口响应时间长排查，通过nginx日志查看接口响应时间**

排查问题

接口响应速度慢，后端打断点进入接口到返回这段时间有很快，要判断是什么问题，首先要确定是前端问题还是后端问题，可通过[nginx](https://so.csdn.net/so/search?q=nginx&spm=1001.2101.3001.7020)日志查看接口响应时间

切换到nginx目录

 cd /var/log/nginx/

查看nginx日志

 tail -f access.log

![img](https://bytedancecampus1.feishu.cn/space/api/box/stream/download/asynccode/?code=MTIzNDhkYmI3NWEwOTdhNDU0Y2JlYmYwOWJjNGIzYmJfejlhU2x4REg0TkRwWWR1MllUVXlBUmlrRXJXNGZLR3JfVG9rZW46Ym94Y25UQWNEamZ6dVhtQTZ2UVprVGtCRFBkXzE2ODgzNTg4NjM6MTY4ODM2MjQ2M19WNA)

红线框是接口响应时间，默认的nginx日志是不会显示接口时间的，想看接口响应时间需要修改日志格式请进nginx日志格式分析及修改

分析

通过查看nginx日志接口响应，发现nginx日志接口响应时间与浏览器network接口响应时间差不多，所以排除了前端问题，后经过是后端排查以及服务性能检测发现，接口在执行sql之后有时候会有一段时间，接下来就需要后端去排查这段时间都做了什么

原文链接：https://blog.csdn.net/meimeib/article/details/124383197

### **lnmp架构如果用户访问过慢应该如何排查，写出解决方案**

- 先自己去测试一下，没有问题就从用户的角度出发。如果有出现同样的问题按照一下思路去排查。

1. 网络层面
   1. 是否ping通
      1. 如果ping域名不通，说明服务宕机了or服务过载了or http服务的问题
      2. 如果ping的通，不丢包。http服务的问题（服务宕机或者服务过载了）
      3. 如果ping的通，丢包，机房宽带稳定，线路不稳定
   2. tracert -d  www.***.com 从客户端到服务器之间各个线路，让机房配合查（Windows cmd）
2. 外部层面
   1. 服务器购买的带宽满了（通过流量监测服务查看）
   2. 内链外链（调用外部网站网址有问题）
3. web服务层面

检查代码层面，代码逻辑是否有问题，是否多线程阻塞，数据库层面：定位慢sql问题

提供自带的可视化监控工具，比如jdk自带的jvisualvm工具监控线程，cpu的运行情况

- 数据库问题，登录数据库，看是否有慢查询语句show processlist，调整MySQL配置，优化SQL语句
  - 一条慢sql如何优化
    - 未命中索引，增加索引
    - explain查看执行计划，命中索引还慢，那就是数据量大，超过500万，需要分库分表，读写分离等
- 存储等的问题。是不是存储服务器，如NFS，MFS的负载及磁盘IO高

1. 机房业务是否OK
2. telnet www. baidu .com 80 检查服务器Web服务有没有开启以及防火墙有没有挡住
3. curl www.*.com 或 wget www. *.com 相当于浏览器访问
4. 提供服务的服务器是否资源过载，服务器连接数过多，负载高，CPU高，IO高等

大题问了：

lnmp架构如果用户访问过慢应该如何排查，写出解决方案

软硬连接的区别

这一个shell脚本，把A目录下以.conf结尾的文件移动到B目录下，并更名为.html

对于管理大量linux服务器的思路和方案

tcp连接的过程

如果着急的话，就是换一个cdn厂商

有网友建议是使用cdnjs或者unpkg,或者bootcdn

比如：https://cdn.jsdelivr.net/npm/换成https://unpkg.com/能救急 仅限npm部分

可以代替的cdn服务：

https://cdnjs.com/

https://www.unpkg.com/

https://www.bootcdn.cn/

原文链接：https://blog.csdn.net/u013014254/article/details/122045331

![img](https://bytedancecampus1.feishu.cn/space/api/box/stream/download/asynccode/?code=MDcxODBiZTc5OWUyZTdjMzljY2RlMjgxZjc5NjE1NTdfR1NGUTd1SDV6U3dsQmZ3UHhVNlRpdmpOdVdZdlZtQmxfVG9rZW46Ym94Y255U0xkVDdHdXFiVVRjdkZQZHVUSTZkXzE2ODgzNTg4NjM6MTY4ODM2MjQ2M19WNA)

## **容器和pod关系**

1个pod可以包含1个或多个容器，可以理解为pod是容器的容器。我们可以通过kubectl descile pod < pod name> -n < namespace _name> 命令查看对应pod的容器信息，或者直接查看yaml源文件。

## **pod**

1. pod是k8s调度的最小单元。
2. 1个pod可以包含1个或多个容器，可以理解为pod是容器集合。
3. pod相当于逻辑主机，每个pod通过describe可以看到都有自己的ip地址。
4. pod内的容器可以共享相同的ip和端口空间。
5. 进入pod后，正常的Iinux命令可以使用，若pod内包含多个容器，进入到每个容器都相当于进入了一个逻辑主机。

## **容器**

1. docker包括三个概念: 镜像(image) ，容器(container) 、仓库(repository)
2. 容器就是镜像运行时的实体，镜像和容器的关系类比与Java面向对象程序设计中的类和实例是一样的概念。
3. 每个容器的文件系统与其他容器是隔离的。
4. 容器可以被创建、启动、停止、删除以及暂停等。
5. 容器的实质是进程，运行于属于自己独立的命名空间。
6. 容器内的存储层是跟随容器变化的，生命周期同容器保持一致。容器删除，则存储层信息丢失。所以存储东西最好使用存储卷(volume)、绑定宿主目录等方式。
7. 容器是应用程序层的一种抽象，将代码和依赖关系打包在- -起，可以多个容器同时运行在同- -台机器上，并与其他容器共享操作系统内核。

## **软硬连接的区别**

对于管理大量linux服务器的思路和方案 ：SaltStack/ansible运维工具库

这一个shell脚本，把A目录下以.conf结尾的文件移动到B目录下，并更名为.html

## **进程、线程和协程的区别**

进程是应用程序的实体，分配操作系统资源的最小单位，拥有自己的内存空间以及堆栈。

线程是内核操作系统CPU调度的基本单位，在进程的内存空间及堆栈上进行执行运行。

协程指的是用户态线程，由用户的应用程序进行调度

## **协程的好处**

1. 切换上下文代价小，协程切换时不用进行系统调用，走内核态的指令。
2. 没有多线程的竞争资源锁问题，多线程竞争资源还会锁内存走系统调用，而协程如果竞争资源直接在当前线程上的进行判断就可以了
3. 内存占用小，只占用2K,线程占用2M

## **GMP模型中的GMP指的是什么**

1. G协程
2. M线程
3. P调度器

其中M可能会自旋，也有可能会休眠，GC会把一些休眠的线程销毁。

### 代码冲突怎么办

 和产生冲突的同事一起沟通，看看去谁的代码，然后提交合并

其他：

1. 奖学金，国家励志奖学金，三好学生。
2. 基于大数据分析的推荐系统，这个推荐系统怎么实现的，有怎么调研吗？
   1. 有几类推荐：
      1. 专家推荐：人工推荐，由资深的专业人士来进行物品的筛选和推荐，需要较多的人力成本
      2. 基于统计的推荐：基于统计信息的推荐（如热门推荐），易于实现，但对用户个性化偏好的描述能力较弱
      3. 基于内容的推荐：通过机器学习的方法去描述内容的特征，并基于内容的特征来发现与之相似的内容
      4. 协同过滤推荐：应用最早和最为成功的推荐方法之一，利用与目标用户相似的用户已有的商品评价信息，来预测目标用户对特定商品的喜好程度
      5. 混合推荐：结合多种推荐算法来提升推荐效果
   2. 推荐系统的技术栈 
   3. ![img](https://bytedancecampus1.feishu.cn/space/api/box/stream/download/asynccode/?code=MzE3MDZiNGU1ZjUxZmQ2OWEzODA4YTBiNDI5N2M2OTJfMkxNTkZXdmpIQ0xWQ3BEeXQ4NzVqZW1BZUh4NGJqSW5fVG9rZW46Ym94Y254cDFVUmZoaEtrRVdKWWxuZktUUmxkXzE2ODgzNTg4NjM6MTY4ODM2MjQ2M19WNA)

   4. 存储系统：Habse是基于HDFS的一种数据库，Hbase的文件是存储在HDFS系统之上的。（解决了数据存储问题）
   5. 采集组件：sqoop（针对RDMS关系型数据库的数据进行采集，订单，交易，还有浏览，收藏的）
   6. flume（实时的，比如用户点击的网页的图片和商品，这种点击流）
   7. kaffka（第三方消息队列框架，可靠性好）
   8. storm：流处理
   9. Yarn：对资源进行分配，基于管理员配置的策略
   10. HIve，Pig，Spark对结构化数据进行存储
   11. MLlib：数据挖掘，实现推荐
   12. Oozie：整体调度
   13. Zookeeper：失败切换，多并发访问，提供请求响应
   14. Ambari：提供图形化
   15. ![img](https://bytedancecampus1.feishu.cn/space/api/box/stream/download/asynccode/?code=OTJmZWEyMTEzYWVhZjliNGUxZDI5YzgyZWMxZTE5ZTZfbEg0SFRndXRiNGFoZG4wcFZoTEFuZlBJaFFiV21ZSEZfVG9rZW46Ym94Y24zUWZmd2xkOG5nVmtWTHJCTTV6SmxoXzE2ODgzNTg4NjM6MTY4ODM2MjQ2M19WNA)

   16. SparkSQL：数据框dataafream
   17. Spark Streaming：RDD，流数据
   18. MLlib：机器学习数据模型库
   19. GraphX：图计算
   20. Packages：各种包
   21. 数据源：hadoop的HDFS，mysql，Json
   22. 除此以外，还需要mySQL，redis，web等

1. 怎么谈薪。流程？
   1. 按照公司的流程来就好。
2. 你八股文是怎么准备的，平时也是背过来的么？
   1. 操作系统-计算机网络-背
3. 二面项目经验怎么hold住面试官
4. 现在有大厂hc，还有必要去大厂实习不？
5. 主动说出痛点：牵头，去改造，解决了。

1. 我们要通过客户的数据的api，导入到客户的api
2. 我们去做了低代码配置，不能解决，
3. 我们原本是做成什么的，不能每一个客户都需要单独。
4. 痛点，通过脚本，调研js,java,这个满足不了业务场景。
5. 通过脚本存储过程去，写库的操作。